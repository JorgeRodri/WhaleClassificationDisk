{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named keras",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-474a6feeeff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;31m# from networks.autoencoder import get_autoencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named keras"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "# from networks.autoencoder import get_autoencoder\n",
    "import keras\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "\n",
    "print('Using Keras 33version', keras.__version__)\n",
    "\n",
    "save_path = 'autoencoder_results/'\n",
    "tag = 'autoenccoder_'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "input_size = [128, 128, 1]\n",
    "batch_size = 300\n",
    "\n",
    "general_path = 'D:/DatasetsTFM/ProjectTrials/'\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator()\n",
    "data_generator = datagen.flow_from_directory(\n",
    "    general_path + '../KaggleData/DIYS128/Train/',\n",
    "    batch_size=batch_size,\n",
    "    target_size=input_size[:-1],\n",
    "    shuffle=True,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode='categorical')\n",
    "\n",
    "val_datagen = ImageDataGenerator()\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    general_path + '../KaggleData/DIYS128/Validation/',\n",
    "    batch_size=batch_size,\n",
    "    target_size=input_size[:-1],\n",
    "    shuffle=False,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=input_shape)  # adapt this if using `channels_first` image data format\n",
    "\n",
    "# nn.add(Conv2D(64, kernel_size=(7, 7), activation='relu', input_shape=input_size))\n",
    "# nn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# nn.add(Conv2D(128, kernel_size=(5, 5), activation='relu'))\n",
    "#\n",
    "# nn.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "x = Conv2D(128, kernel_size=(5, 5), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(64, kernel_size=(5, 5), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "print(\"shape of encoded\", K.int_shape(encoded))\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(64, (5, 5), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(128, (5, 5), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "print(\"shape of decoded\", K.int_shape(decoded))\n",
    "\n",
    "\n",
    "return Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# opt = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "# nn.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy']) #default\n",
    "# autoencoder.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit_generator(data_generator,\n",
    "                        samples_per_epoch=7000,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=10,\n",
    "                        nb_epoch=9,\n",
    "                        verbose=2)\n",
    "\n",
    "\n",
    "score = autoencoder.evaluate_generator(val_gen, steps=10)\n",
    "\n",
    "print(score)\n",
    "\n",
    "\n",
    "##Store Plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','validation'], loc='upper left')\n",
    "plt.savefig(save_path + tag + 'model_accuracy' + str(score[1]) + '.pdf')\n",
    "plt.close()\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.savefig(save_path + tag + 'model_loss' + str(score[1]) + '.pdf')\n",
    "\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator()\n",
    "data_generator = datagen.flow_from_directory(\n",
    "    '../KaggleData/DIYS2/Train',\n",
    "    batch_size=batch_size,\n",
    "    target_size=input_size[:-1],\n",
    "    shuffle=False,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "y_train = data_generator.classes\n",
    "Y_pred = autoencoder.predict_generator(data_generator, steps=70)\n",
    "# print(Y_pred.shape)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print('Analysis of results')\n",
    "target_names = ['no_whale', 'whale']\n",
    "print(classification_report(y_train, y_pred, target_names=target_names))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "#Compute probabilities\n",
    "y_test = val_gen.classes\n",
    "Y_pred = autoencoder.predict_generator(val_gen, steps=10)\n",
    "# print(Y_pred.shape)\n",
    "#Assign most probable label\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#Plot statistics\n",
    "print('Analysis of results')\n",
    "target_names = ['no_whale', 'whale']\n",
    "print(classification_report(y_test, y_pred,target_names=target_names))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "#Saving model and weights\n",
    "from keras.models import model_from_json\n",
    "nn_json = autoencoder.to_json()\n",
    "with open(save_path + 'nn_' + tag +str(score[1])+ '.json', 'w') as json_file:\n",
    "    json_file.write(nn_json)\n",
    "weights_file = \"weights_\"+ tag +str(score[1])+\".hdf5\"\n",
    "autoencoder.save_weights(save_path + weights_file, overwrite=True)\n",
    "\n",
    "#Loading model and weights\n",
    "# json_file = open(save_path + 'nn' + str(score[1]) + '.json','r')\n",
    "# nn_json = json_file.read()\n",
    "# json_file.close()\n",
    "# nn = model_from_json(nn_json)\n",
    "# nn.load_weights(save_path + weights_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}